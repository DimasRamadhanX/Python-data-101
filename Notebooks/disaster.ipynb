{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def create_svm_model(input_dim, num_classes, n_support_vectors=10, kernel='rbf', C=1.0):\n",
    "#     \"\"\"\n",
    "#     Create a PyTorch-based SVM model with all necessary components\n",
    "#     \"\"\"\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "#     # Create model parameters\n",
    "#     model_params = {\n",
    "#         'support_vectors': torch.randn(n_support_vectors, input_dim, device=device, requires_grad=True),\n",
    "#         'dual_coef': torch.randn(n_support_vectors, num_classes, device=device, requires_grad=True),\n",
    "#         'intercept': torch.zeros(num_classes, device=device, requires_grad=True)\n",
    "#     }\n",
    "    \n",
    "#     def kernel_function(x1, x2, kernel_type=kernel):\n",
    "#         if kernel_type == 'linear':\n",
    "#             return torch.mm(x1, x2.t())\n",
    "#         elif kernel_type == 'rbf':\n",
    "#             distances = torch.cdist(x1, x2, p=2).pow(2)\n",
    "#             gamma = 1.0 / x1.shape[1]\n",
    "#             return torch.exp(-gamma * distances)\n",
    "#         elif kernel_type == 'poly':\n",
    "#             return (torch.mm(x1, x2.t()) + 1).pow(3)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported kernel type\")\n",
    "    \n",
    "#     def forward_pass(x):\n",
    "#         kernel_matrix = kernel_function(x, model_params['support_vectors'])\n",
    "#         return torch.mm(kernel_matrix, model_params['dual_coef']) + model_params['intercept']\n",
    "    \n",
    "#     return forward_pass, model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba=None):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            # For multiclass ROC AUC\n",
    "            lb = LabelBinarizer()\n",
    "            y_true_bin = lb.fit_transform(y_true)\n",
    "            if y_pred_proba.shape[1] == 2:  # Binary classification\n",
    "                metrics['roc_auc'] = roc_auc_score(y_true_bin, y_pred_proba[:, 1])\n",
    "            else:  # Multi-class\n",
    "                metrics['roc_auc'] = roc_auc_score(\n",
    "                    y_true_bin, \n",
    "                    y_pred_proba,\n",
    "                    multi_class='ovr',\n",
    "                    average='weighted'\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not calculate ROC AUC: {str(e)}\")\n",
    "            metrics['roc_auc'] = None\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(model, X_train, y_train, skf, learning_rate=0.001, epochs=100):\n",
    "    fold_results = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # Buat model baru untuk setiap fold\n",
    "        model = bpnn(num_hidden_layers=2, num_neurons=64, activation_functions={\n",
    "            'hidden': 'sigmoid',\n",
    "            'output': 'sigmoid'\n",
    "        }).to(device)\n",
    "\n",
    "        # Definisikan loss function dan optimizer\n",
    "        criterion = nn.BCELoss()  # Sesuaikan jika output bukan binary\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Pelatihan\n",
    "        for epoch in range(epochs):\n",
    "            model.train()  # Set model ke mode training\n",
    "            optimizer.zero_grad()  # Reset gradien\n",
    "            outputs = model(torch.tensor(X_fold_train, dtype=torch.float32).to(device))  # Forward pass\n",
    "            loss = criterion(outputs, torch.tensor(y_fold_train, dtype=torch.float32).to(device).view(-1, 1))  # Hitung loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update parameter\n",
    "\n",
    "        # Evaluasi pada validation set\n",
    "        model.eval()  # Set model ke mode evaluasi\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(torch.tensor(X_fold_val, dtype=torch.float32).to(device))  # Forward pass\n",
    "            val_loss = criterion(val_outputs, torch.tensor(y_fold_val, dtype=torch.float32).to(device).view(-1, 1))\n",
    "\n",
    "        fold_results.append(val_loss.item())  # Simpan hasil loss fold ini\n",
    "\n",
    "    # Kembalikan rata-rata dari hasil cross-validation\n",
    "    return np.mean(fold_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpnn(num_hidden_layers, num_neurons, activation_functions):\n",
    "    # Tentukan perangkat (GPU jika tersedia)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Tentukan arsitektur jaringan\n",
    "    layers = []\n",
    "\n",
    "    # Tambahkan input layer\n",
    "    layers.append(nn.Linear(num_neurons, num_neurons))  # 'input_dim' harus ditentukan di cross_validation\n",
    "    if activation_functions['hidden'] == 'relu':\n",
    "        layers.append(nn.ReLU())\n",
    "    elif activation_functions['hidden'] == 'sigmoid':\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "    # Tambahkan hidden layers\n",
    "    for _ in range(1, num_hidden_layers):\n",
    "        layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "        if activation_functions['hidden'] == 'relu':\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation_functions['hidden'] == 'sigmoid':\n",
    "            layers.append(nn.Sigmoid())\n",
    "\n",
    "    # Tambahkan output layer\n",
    "    layers.append(nn.Linear(num_neurons, 1))\n",
    "    if activation_functions['output'] == 'sigmoid':\n",
    "        layers.append(nn.Sigmoid())\n",
    "    elif activation_functions['output'] == 'softmax':\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "    # Buat model dan pindahkan ke perangkat\n",
    "    model = nn.Sequential(*layers).to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create sample data\n",
    "df=pd.read_csv(\"diabetes_012_health_indicators_BRFSS2015.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diabetes_012\n",
       "0    213703\n",
       "2     35346\n",
       "1      4631\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Diabetes_012'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "253675    0\n",
       "253676    2\n",
       "253677    0\n",
       "253678    0\n",
       "253679    2\n",
       "Name: Diabetes_012, Length: 253680, dtype: int8"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Diabetes_012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns='Diabetes_012'))\n",
    "y = np.array(df['Diabetes_012'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "undersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Lakukan undersampling\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "# Create cross-validation splitter\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Get dimensions from data\n",
    "input_dim = X.shape[1]\n",
    "num_classes = len(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (162355x21 and 64x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m activation_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# atau 'relu'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# atau 'softmax'\u001b[39;00m\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m bpnn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m64\u001b[39m,activation_functions)\n\u001b[1;32m----> 7\u001b[0m mean_metrics\u001b[38;5;241m=\u001b[39m\u001b[43mrun_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mskf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(mean_metrics\u001b[38;5;241m.\u001b[39mitems(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics_df)\n",
      "Cell \u001b[1;32mIn[44], line 22\u001b[0m, in \u001b[0;36mrun_cross_validation\u001b[1;34m(model, X_train, y_train, skf, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model ke mode training\u001b[39;00m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset gradien\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fold_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, torch\u001b[38;5;241m.\u001b[39mtensor(y_fold_train, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Hitung loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luxha\\anaconda3\\envs\\rapids\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luxha\\anaconda3\\envs\\rapids\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luxha\\anaconda3\\envs\\rapids\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\luxha\\anaconda3\\envs\\rapids\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\luxha\\anaconda3\\envs\\rapids\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\luxha\\anaconda3\\envs\\rapids\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (162355x21 and 64x64)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run cross-validation\n",
    "activation_functions = {\n",
    "    'hidden': 'sigmoid',  # atau 'relu'\n",
    "    'output': 'sigmoid'   # atau 'softmax'\n",
    "}\n",
    "model = bpnn(2,64,activation_functions)\n",
    "mean_metrics=run_cross_validation(model, X_train, y_train,skf)\n",
    "metrics_df = pd.DataFrame(mean_metrics.items(), columns=['Metric', 'Value'])\n",
    "\n",
    "print(metrics_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
